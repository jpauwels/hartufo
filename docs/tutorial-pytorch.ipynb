{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using an HRTF Dataset with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hartufo.torch import collate_dict_dataset\n",
    "from hartufo import CipicPlane, AriPlane\n",
    "from hartufo import CollectionSpec\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../hartufo-collections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All instances of `hartufo.Dataset` have a class interface that is directly compatible with PyTorch [`Datasets`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), specifically [map-style datasets](https://pytorch.org/docs/stable/data.html#map-style-datasets). They can therefore be directly used to create a a [`torch.util.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). However, because `hartufo.Dataset` returns datapoints with a dict format (to allow advanced dataset splitting strategies based on indivisible groups), the provided `collate_dict_dataset` collation function is required when creating a `DataLoader` to convert the dataset into expected `(feature, target)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipic_ds = CipicPlane(base_dir / 'CIPIC', 'horizontal', 'magnitude_db', 'both-left', plane_angles = (-30, 0, 30),\n",
    "                      other_specs={'target_spec': CollectionSpec()}, subject_ids=(3, 8))\n",
    "cipic_loader = DataLoader(cipic_ds, collate_fn=collate_dict_dataset)\n",
    "features, target = next(iter(cipic_loader))\n",
    "features.shape, target, len(cipic_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the compatible interface of `hartufo.Dataset`, other `torch.util.data` functionality can be used too, to [chain](https://pytorch.org/docs/stable/data.html#torch.utils.data.ChainDataset) or [concatenate](https://pytorch.org/docs/stable/data.html#torch.utils.data.ConcatDataset) datasets, take [subsets](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) or use [custom samplers](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler), for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_ds = AriPlane(base_dir / 'ARI', 'horizontal', 'magnitude_db', 'both-left', plane_angles = (-30, 0, 30),\n",
    "                  other_specs={'target_spec': CollectionSpec()}, subject_ids=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds = ConcatDataset([cipic_ds, ari_ds])\n",
    "for ex in combined_ds:\n",
    "    print(ex['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compatibility of `hartufo.Dataset` with `torch.util.data.Dataset` means that there are two possible approaches to choose from when splitting a dataset. You can either specify a `subject_ids` argument during the construction of an `hartufo.Dataset` or use `torch.util.data.Subset` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former strategy has the advantage that you can use subject ids to specify the split instead of sequential indices (and remember that each subject can provide to one or two datapoint depending on the choice of `side`). The latter strategy is advisable when a single dataset is repeatedly split, such as during K-fold cross-validation, because this way all datapoints are read only once from disk whereas the former strategy would lead to reading each datapoint K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipic_selected_ids = CipicPlane(base_dir / 'CIPIC', 'horizontal', 'magnitude_db', 'both-left', plane_angles = (-30, 0, 30),\n",
    "                                other_specs={'target_spec': CollectionSpec()}, subject_ids=(8,)) # reads again from disk\n",
    "cipic_subset = Subset(cipic_ds, [2, 3]) # uses dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cipic_selected_ids[-1]['features'] == cipic_subset[-1]['features']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in case of contiguous splits, it is also possible index the dataset directly, but this is not always applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipic_indexed = cipic_ds[2:4]\n",
    "(cipic_indexed['features'][-1] == cipic_subset[-1]['features']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6da6f4d86ef4f519a6c275da41ee9fe88a8dc4690964d65fd688f7f44823bf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
