{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Creating and Processing Datasets with Arbitrary Source Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hartufo import Cipic, Ari, Listen, BiLi, Ita, Hutubs, Riec, Sadie2, Princeton3D3A, Chedar, Widespread, Sonicom\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../HRTF Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you have completed the tutorial \"Using a dataset of planes in PyTorch\" (available at `docs/tutorial-plane.ipynb`), so go read that first if you haven't done so already.\n",
    "\n",
    "As mentioned in the previous tutorial, the purpose of `hartufo` is to provide a unified programming interface for multiple collections of HRTFs. What is not immediately apparent from using the `XPlane` classes, is that you can do more than simply load predefined datasets. The idea behind `hartufo` is that you can select any information contained in an HRTF data collection and combine them into a dataset with your desired characteristics. It can be helpful to think of the classes as `Dataset` _generators_, not just data _loaders_.\n",
    "\n",
    "The same data collections as for the plane datasets are available for use:\n",
    "- CIPIC\n",
    "- ARI\n",
    "- Listen\n",
    "- BiLi\n",
    "- ITA\n",
    "- HUTUBS\n",
    "- RIEC\n",
    "- SADIE II\n",
    "- 3D3A\n",
    "- CHEDAR\n",
    "- Widespread\n",
    "- SONICOM\n",
    "\n",
    "Each of them has a corresponding class that can be loaded from `hartufo`, as is done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of \"specifications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Dataset`, you need to define its \"specification\", meaning that you need to tell what information you want to use as `features` and optionally what as `target` labels and/or as `groups`. Each of these specifications takes the form of a Python dictionary with any combination of the following keys:\n",
    "\n",
    "- collection: a string identifier of the name of the data collection\n",
    "- subject: an integer giving the identifier of the subject in the collection\n",
    "- side: a string indicating the side of the head \n",
    "- hrirs: an array containing the HRIRs for the given positions and side of the head\n",
    "\n",
    "These four pieces of information are available for every collection. The following keys (and corresponding information) are sometimes available, depending on the collection, and will be discussed in a later tutorial:\n",
    "\n",
    "- image: an array containing the pixel values of an image associated with a given side of the head \n",
    "- 3d-model: an array containing a 3D model\n",
    "- anthropometry: an array containing anthropometric measurements\n",
    "\n",
    "The values for each of these keys are also Python dictionaries themselves, and allow to pass parameters for each type of info. The first three types `collection`, `subject` and `side` have no parameters, so an empty dictionary `{}` should be passed as value for these keys. The `hrir` type does allow to pass parameters in the form of a dictionary with the following keys:\n",
    "\n",
    "- domain: the HRIR/HRTF representation (`time`, `magnitude`, `magnitude_db`, `phase` or `complex`, default: `time`) \n",
    "- side: selecting a side of the head (`left`, `right`, `both`, `both-left`, `both-right`, `None`, default: `None` meaning any side available)\n",
    "- fundamental_angles: a list of angles in degrees to select from the fundamental coordinate plane (explained later), default: all available\n",
    "- orthogonal_angles: a list of angles in degrees to select from the orthogonal coordinate plane (explained later), default: all available\n",
    "- additive_scale_factor: a factor to add to all HRIRs, default: 0\n",
    "- multiplicative_scale_factor: a factor to multiply all HRIRs with, default: 1\n",
    "- samplerate: a target sample rate to resample all HRIRs to, default: no resampling\n",
    "- length: an uppper limit to the number of samples in a HRIR (after resampling), beyond wich samples get truncated, default: no truncation\n",
    "- min_phase: a boolean flag to indicate whether the minimum phase HRIR should be used, default: False\n",
    "\n",
    "An example should make this clearer: a dataset is created below that has left-ear magnitude HRTFs as features, expressed in decibels. The target labels are set to the side of the head and the grouping is determined by the subject ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "domain = 'magnitude_db'\n",
    "side = 'left'\n",
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain}}, target_spec={'side': {}}, group_spec={'subject': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset has a large part of the functionality of plane datasets, excluding plotting. So you can get its length, index individual datapoints as dictionaries and check their keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds), ds[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the `target` and `group` value indeed contain the side of the head and the subject id of the data point, as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['target'], ds[0]['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you can get the sample rate of the HRIR and its corresponding HRTF frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.hrir_samplerate, ds.hrtf_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ids of all available subjects and of this particular selection (all by default) can also be obtained using the properties `available_subject_ids` and `subject_ids`. The actual selection can be achieved by passing the argument `subject_ids` to the dataset constructor, just like for plane datasets (including the `first`, `last` and `random` options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain}}, target_spec={'side': {}}, group_spec={'subject': {}}, subject_ids='random')\n",
    "ds.available_subject_ids[:10], ds.subject_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More specification examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `features_spec` is obligatory, `target_spec` and/or `group_spec` can be absent, leading to empty values, which is especially useful for situations where no target labels are needed (e.g. when using unsupervised learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain}}, subject_ids='first')\n",
    "ds[0]['target'], ds[0]['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple elements of information can be given in a `spec`, which will then be combined into one multi-dimensional element, for instance a target label consisting of three parts as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {}}, target_spec={'collection': {}, 'subject': {}, 'side': {}}, subject_ids='first')\n",
    "ds[0]['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HRIRs do not necessarily need to be used as features they can be used in any spec just like any other type of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'side': {}}, target_spec={'hrirs': {}}, subject_ids='first')\n",
    "ds[0]['features'], ds[0]['target'].shape, ds[0]['target'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dtype of numerical values is `np.float32` by default, but this can be changed by passing a `dtype` argument to the constructor. When requesting an HRTF in the `complex` domain, it is necessary to specify a complex dtype too, otherwise an error will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'domain': 'complex'}}, subject_ids='first')\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'domain': 'complex'}}, subject_ids='first', dtype=np.complex64)\n",
    "ds[0]['features'].shape, ds[0]['features'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout of HRIR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of where the HRIRs are used, they are always stored as a 3-dimensional array. In order to describe the data layout independently of coordinate system, we need to define two terms. The _fundamental_ plane of a coordinate system is the reference plane onto which each point gets projected. Its angles span a range of 360 degrees. The _orthogonal_ plane of a coordinate system is the plane defined by a point, its projection and the origin. Its angles span a range of 180 degrees. In practice, the fundamental plane will nearly always be the horizontal plane because only the CIPIC collection uses interaural-polar coordinates, all other supported collections use vertical-polar coordinates.\n",
    "\n",
    "The various spatial positions for which a HRIR measurement is available are organised along the rows and columns of the 3D array. The values of a single HRIR/HRTF (depending on the selected `domain`) for a particular position determined by row and column index are stored in the third dimension. The rows are ordered by increasing angle in the fundamental plane, constrained to the interval [-180, 180) (so vertical angles for CIPIC, azimuths for all the rest). The columns of the array are ordered by increasing angle in the orthogonal plane, constrained to the interval [-90, 90] (so lateral angles for CIPIC, elevations for all the rest).\n",
    "\n",
    "This means that the spherical positions are stored as a [plate carrée](https://en.wikipedia.org/wiki/Equirectangular_projection) projection. The first and last column are always the positions closest to the poles, regardless of the coordinate system, although the measurements for the poles are not necessarily present (in the collection or just in a particular subselection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain}}, target_spec={'side': {}}, group_spec={'subject': {}}, subject_ids='first')\n",
    "ds[0]['features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual values of the angles can be read from the `fundamental_angles` and `orthogonal_angles` properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(ds.fundamental_angles), len(ds.orthogonal_angles)), ds.fundamental_angles, ds.orthogonal_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all positions available in a collection are read, but a selection can be made by adding them to the `spec` as an iterable of angles. A value of `None` signifies all available, the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain, 'fundamental_angles': None, 'orthogonal_angles': None}}, subject_ids='first')\n",
    "ds[0]['features'].shape, ds.fundamental_angles, ds.orthogonal_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine a value of `None` for either `fundamental_angles` or `orthogonal_angles` with an iterable of one or more angles, in which case the subset of all row/column angles with the given column/row angle(s) will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain, 'fundamental_angles': (0,), 'orthogonal_angles': None}}, subject_ids='first')\n",
    "ds[0]['features'].shape, ds.fundamental_angles, ds.orthogonal_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both `fundamental_angles` and `orthogonal_angles` are given as iterables, they need to be of the same length and all spatial positions determined by the pairs of angles with equal index are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "angle_pairs = np.array(tuple(product(np.arange(-180, 180, 20), (-30, 0, 30))))\n",
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain, 'fundamental_angles': angle_pairs[:, 0], 'orthogonal_angles': angle_pairs[:, 1]}}, subject_ids='first')\n",
    "ds[0]['features'].shape, ds.fundamental_angles, ds.orthogonal_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requesting an angle that is not available in the collection will be silently ignored. So when using the positional values in further processing, it is advised to use the exact angles that are returned by reading the relevant properties instead of using the requested values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_fundamental_angles = (0, 0)\n",
    "requested_orthogonal_angles = (0, 3.14)\n",
    "ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain, 'fundamental_angles': requested_fundamental_angles, 'orthogonal_angles': requested_orthogonal_angles}}, subject_ids='first')\n",
    "ds[0]['features'].shape, ds.fundamental_angles, ds.orthogonal_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(requested_fundamental_angles) == ds.fundamental_angles).all(), (np.unique(requested_orthogonal_angles) == ds.orthogonal_angles).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only when none of the requested angle pairs are available, will an exception be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'side': side, 'domain': domain, 'fundamental_angles': (0,), 'orthogonal_angles': (3.14,)}}, subject_ids='first')\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of the row and column angles as forming a matrix of available positions, then this matrix is not necessarily dense. Not every collection has chosen a spatial sampling where every row/column angle combination is measured. Consequently, the 3D array of HRIRs is stored as a [NumPy masked array](https://numpy.org/doc/stable/reference/maskedarray.html), where row/column combinations that have not been measured are masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the distribution of the spatial positions by plotting the mask of a collection (technically a single, third dimension slice of the mask but all slices are the same since either all samples of a HRIR are present or none). Do note that the masks are displayed by position index, not actual angles, and that the angular sampling is not necessarily regular. Therefore these plots cannot be interpreted as representations of the angular distribution of the measurement positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = (\n",
    "    (Cipic, base_dir / 'CIPIC'),\n",
    "    (Ari, base_dir / 'ARI'),\n",
    "    (Listen, base_dir / 'Ircam Listen'),\n",
    "    (BiLi, base_dir / 'Ircam BiLi'),\n",
    "    (Ita, base_dir / 'ITA Aachen'), \n",
    "    (Hutubs, base_dir / 'HUTUBS'),\n",
    "    (Riec, base_dir / 'RIEC'),\n",
    "    (Sadie2, base_dir / 'SADIE II'),\n",
    "    (Princeton3D3A, base_dir / '3D3A'),\n",
    "    (Chedar, base_dir / 'CHEDAR'),\n",
    "    (Widespread, base_dir / 'Widespread'),\n",
    "    (Sonicom, base_dir / 'SONICOM'),\n",
    ")\n",
    "\n",
    "from math import ceil\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for idx, (collection, data_dir) in enumerate(collections):\n",
    "    ds = collection(data_dir,  features_spec={'hrirs': {}}, subject_ids='first')\n",
    "    ax = fig.add_subplot(2, ceil(len(collections)/2), idx+1)\n",
    "    ax.set_title(collection.__name__)\n",
    "    ax.matshow(np.ma.getmaskarray(ds[0]['features'][:, :, -1, 0]))\n",
    "    ax.set_aspect(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing HRIRs further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to process the HRIRs further without needing to modify any internal code. To that end, a callable can be passed as value for the key `preprocess` or `transform` in the spec. The callable itself takes one argument, the 3D array of HRIRs and needs to return the modified array. Both callables passed to `preprocess` and `transform` will produce the same result, the only difference is when the callable gets executed. A callable passed to `preprocess` will be run during the construction of the dataset. This means it will be executed once and its output will be stored as the values of the dataset. For instance, if you `pickle` a dataset to disk, then load it again, it is the output of the callable that is loaded, not the raw data. On the other hand, callables passed to `transform` are run every time a datapoint is accessed, e.g. through indexing or slicing. This means that the output of the callable is returned but never stored, the underlying data gets preserved. Therefore the latter is suitable for stochastic transformations, whereas the former is strictly deterministic.\n",
    "\n",
    "As an example, suppose that the decibel conversion of the HRTF magnitudes was not built-in, then it could be added by passing the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_calc(hrtfs):\n",
    "    return 20*np.log10(np.clip(hrtfs, 1e-9, None))\n",
    "\n",
    "external_db_calc = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'domain': 'magnitude', 'preprocess': db_calc}}, subject_ids='first')\n",
    "builtin_db_calc = Ari(base_dir / 'ARI',  features_spec={'hrirs': {'domain': 'magnitude_db'}}, subject_ids='first')\n",
    "np.allclose(external_db_calc.features, builtin_db_calc.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned HRIRs do not necessarily need to have the same shape as the input. For instance, this is done by the `XPlane` classes, which return 2D planes. Internally, the `plane` argument of the constructor is converted into a set of row and column angles, which get read and subsequently stitched together into a single plane to form the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6da6f4d86ef4f519a6c275da41ee9fe88a8dc4690964d65fd688f7f44823bf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
